{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import pickle\n",
    "import imageio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import copy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "# 读取数据\n",
    "wafer_data = pd.read_pickle(\"LSWMD.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:56: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n"
     ]
    }
   ],
   "source": [
    "# 提取不同类型数据\n",
    "wafer_data = wafer_data.drop(['waferIndex'], axis = 1)\n",
    "noType = wafer_data[wafer_data.failureType.str.len() == 0]\n",
    "noneType = wafer_data[wafer_data.failureType == 'none']\n",
    "wafer_data = wafer_data[wafer_data.failureType.str.len() > 0]\n",
    "hasType = wafer_data[wafer_data.failureType != 'none']\n",
    "hasType = hasType.reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 48)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noneType.waferMap[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor转图片\n",
    "loader = transforms.Compose([\n",
    "    transforms.ToTensor()])  \n",
    "\n",
    "unloader = transforms.ToPILImage()\n",
    "\n",
    "def tensor_to_PIL(tensor):\n",
    "    image = tensor.cpu().clone()\n",
    "    image = image.squeeze(0)\n",
    "    image = unloader(image)\n",
    "    return image\n",
    "\n",
    "# 图片转tensor\n",
    "transform1 = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新调整图片大小\n",
    "scaledimgs = []\n",
    "\n",
    "for y in range(len(hasType)):\n",
    "    img = Image.fromarray(hasType.waferMap[y])\n",
    "    scimg = img.resize((32, 32), Image.ANTIALIAS)\n",
    "    npscimg = np.array(scimg)\n",
    "    scaledimgs.append(npscimg)\n",
    "\n",
    "scaledimgs = np.array(scaledimgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledimgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traincount:  17625\n",
      "testcount:  7894\n",
      "allcount:  0\n"
     ]
    }
   ],
   "source": [
    "# 分开训练集和测试集的标签\n",
    "scaledtrain = []\n",
    "scaledtest = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "scall = []\n",
    "y_all = []\n",
    "traincount = 0\n",
    "testcount = 0\n",
    "allcount = 0\n",
    "\n",
    "for y in range(len(hasType)):\n",
    "#     scall.append(scaledimgs[y])\n",
    "#     allcount += 1\n",
    "#     y_all.append(hasType.failureType[y][:])\n",
    "    if(hasType.trianTestLabel[y] == 'Training'):\n",
    "        #if(type(scaledimgs[y]) is not int):\n",
    "        scaledtrain.append(scaledimgs[y])\n",
    "        traincount += 1\n",
    "        y_train.append(hasType.failureType[y][:])\n",
    "    elif(hasType.trianTestLabel[y] == 'Test'):\n",
    "        scaledtest.append(scaledimgs[y])\n",
    "        testcount += 1\n",
    "        y_test.append(hasType.failureType[y][:])\n",
    "\n",
    "nptrain = np.array(scaledtrain)\n",
    "# npy_train = np.array(y_train)\n",
    "\n",
    "nptest = np.array(scaledtest)\n",
    "# npy_test = np.array(y_test)\n",
    "\n",
    "# npall = np.array(scall)\n",
    "# # npy = np.array(y_all)\n",
    "\n",
    "print(\"traincount: \", traincount)\n",
    "print(\"testcount: \", testcount)\n",
    "print(\"allcount: \", allcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Loc'],\n",
       "       ['Edge-Loc'],\n",
       "       ['Edge-Loc'],\n",
       "       ...,\n",
       "       ['Scratch'],\n",
       "       ['Edge-Loc'],\n",
       "       ['Loc']], dtype='<U9')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标签one hot编码\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# fy_all = np.vstack(y_all)\n",
    "fy_train = np.vstack(y_train)\n",
    "fy_test = np.vstack(y_test)\n",
    "\n",
    "one_hot = preprocessing.OneHotEncoder(sparse = False)\n",
    "\n",
    "train_onehot = one_hot.fit_transform(fy_train)\n",
    "test_onehot = one_hot.fit_transform(fy_test)\n",
    "# all_onehot = one_hot.fit_transform(fy_all)\n",
    "num_classes = len(train_onehot[0])\n",
    "\n",
    "train_onehot = np.array(train_onehot)\n",
    "test_onehot = np.array(test_onehot)\n",
    "# all_onehot = np.array(all_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in nptrain:\n",
    "    for i in range(32):\n",
    "        for j in range(32):\n",
    "            if x[i][j] < 1.5:\n",
    "                x[i][j] = 0\n",
    "            else:\n",
    "                x[i][j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in nptest:\n",
    "    for i in range(32):\n",
    "        for j in range(32):\n",
    "            if x[i][j] < 1.5:\n",
    "                x[i][j] = 0\n",
    "            else:\n",
    "                x[i][j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in npall:\n",
    "    for i in range(32):\n",
    "        for j in range(32):\n",
    "            if x[i][j] < 1.5:\n",
    "                x[i][j] = 0\n",
    "            else:\n",
    "                x[i][j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]])\n",
    "b = np.array([[1],[2],[3],[4]])\n",
    "np.random.seed(199)\n",
    "np.random.shuffle(a)\n",
    "np.random.seed(199)\n",
    "np.random.shuffle(b)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(200)\n",
    "np.random.shuffle(nptrain)\n",
    "np.random.seed(200)\n",
    "np.random.shuffle(train_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nptrain[0][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整理训练数据\n",
    "x_train_tensor = torch.from_numpy(nptrain)\n",
    "y_train_tensor = torch.from_numpy(train_onehot)\n",
    "x_train_tensor = x_train_tensor.reshape([17625,1,32,32])\n",
    "\n",
    "\n",
    "x_test_tensor = torch.from_numpy(nptest[:200])\n",
    "y_test_tensor = torch.from_numpy(test_onehot[:200])\n",
    "x_test_tensor = x_test_tensor.reshape([200,1,32,32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all_tensor = torch.from_numpy(npall/2)\n",
    "y_all_tensor = torch.from_numpy(all_onehot)\n",
    "x_all_tensor = x_all_tensor.reshape([25519,1,32,32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor[0][0][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset_test = torch.utils.data.TensorDataset(x_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset_test[199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset_all = torch.utils.data.TensorDataset(x_all_tensor, y_all_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset_all[0][0][0][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(1,16*2, 3, 2, 1)\n",
    "conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self, d=128):\n",
    "        super(classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, d*2, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(d*2, d*4, 4, 2, 1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(d*4)\n",
    "        self.Linear1 = nn.Linear(d*256, 1024)\n",
    "        self.Linear2 = nn.Linear(1024, 8)\n",
    "\n",
    "    # weight_init\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, input):\n",
    "#         x = F.leaky_relu(self.conv1(input), 0.2)\n",
    "#         x = F.leaky_relu(self.conv2_bn(self.conv2(x)), 0.2)\n",
    "#         x = x.view(-1,64*256)\n",
    "#         x = F.leaky_relu(self.Linear1(x), 0.2)\n",
    "#         x = F.softmax(self.Linear2(x), dim=1)\n",
    "        x = F.relu(self.conv1(input))\n",
    "        x = F.relu(self.conv2_bn(self.conv2(x)))\n",
    "        x = x.view(-1,32*256)\n",
    "        x = F.relu(self.Linear1(x))\n",
    "        x = F.softmax(self.Linear2(x), dim=1)\n",
    "  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2,3.],[2,3,1.]])\n",
    "s = F.softmax(a, dim=1)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_init(m, mean, std):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        m.bias.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "batch_size = 32\n",
    "lr = 0.0002\n",
    "train_epoch = 5\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    #datasets.MNIST('data', train=True, download=True, transform=transform),\n",
    "    torch_dataset,\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    #datasets.MNIST('data', train=True, download=True, transform=transform),\n",
    "    torch_dataset_test,\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "testloader = test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_loader, 0):\n",
    "    inputs, labels = data\n",
    "    print(inputs)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = classifier(32)\n",
    "C.weight_init(mean=0.0, std=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross Entropy loss\n",
    "BCE_loss = nn.BCELoss()\n",
    "CE_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam optimizer\n",
    "C_optimizer = optim.Adam(C.parameters(), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = x_test_tensor[:1000]\n",
    "test_y = y_test_tensor[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sortmax 结果转 onehot\n",
    "def props_to_onehot(props):\n",
    "    if isinstance(props, list):\n",
    "        props = np.array(props)\n",
    "    a = np.argmax(props, axis=1)\n",
    "    b = np.zeros((len(a), props.shape[1]))\n",
    "    b[np.arange(len(a)), a] = 1\n",
    "    return b\n",
    "\n",
    "# a = props_to_onehot(test_output.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "Epoch:  0 | train loss: 1.2740 | test accuracy: 0.61\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "Epoch:  1 | train loss: 1.2757 | test accuracy: 0.52\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-958cc5304d78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#         break\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mC_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m           \u001b[1;31m# clear gradients for this training step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m                 \u001b[1;31m# backpropagation, compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mC_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m                \u001b[1;31m# apply gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_iter = 0\n",
    "\n",
    "for epoch in range(train_epoch):\n",
    "    for b_x, b_y in train_loader:   # gives batch data, normalize x when iterate train_loader\n",
    "        C.train()\n",
    "        fb_x = b_x.float()\n",
    "        output = C(fb_x)       # cnn output\n",
    "        output = output.to(torch.float64)\n",
    "#         print(\"output: \")\n",
    "#         print(output)\n",
    "#         print(b_y)\n",
    "       # loss = BCE_loss(output, b_y)\n",
    "        b_y = torch.topk(b_y, 1)[1].squeeze(1)\n",
    "        loss = CE_loss(output, b_y)\n",
    "#         print(\"loss: \")\n",
    "#         print(loss)\n",
    "#         break\n",
    "        C_optimizer.zero_grad()           # clear gradients for this training step\n",
    "        loss.backward()                 # backpropagation, compute gradients\n",
    "        C_optimizer.step()                # apply gradients\n",
    "        \n",
    "        num_iter += 1\n",
    "        if num_iter%50 == 0:\n",
    "            print(num_iter)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        C.eval()\n",
    "#             npout = props_to_onehot(output.detach().numpy())\n",
    "#             accuracy = float(np.sum((npout == b_y.data.numpy()).sum(axis=1) == 8))/ float(b_y.size(0))\n",
    "#             print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.2f' % accuracy)\n",
    "        ftest_x = test_x.float()\n",
    "        test_output = C(ftest_x)\n",
    "        npout = props_to_onehot(test_output.detach().numpy())\n",
    "        accuracy = float(np.sum((npout == test_y.data.numpy()).sum(axis=1) == 8))/ float(test_y.size(0))\n",
    "        print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.2f' % accuracy)\n",
    "            \n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(((props_to_onehot(C(x_test_tensor.float()).detach().numpy()) == y_test_tensor.data.numpy()).sum(axis=1)) == 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum((npout == test_y.data.numpy()).sum(axis=1) == 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = C(x_test_tensor)\n",
    "#print(test_output)\n",
    "npout = props_to_onehot(test_output.detach().numpy())\n",
    "#pred_y = torch.max(, 1)[1].data.numpy()\n",
    "#print(test_y)\n",
    "accuracy = float(np.sum((npout == y_test_tensor.data.numpy()).sum(axis=1) == 8))/ float(y_test_tensor.size(0))\n",
    "print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.2f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = resnet50.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(resnet50.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_tensor[0][0][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel)\n",
    "        )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inchannel != outchannel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outchannel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, ResidualBlock, num_classes=8):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inchannel = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer1 = self.make_layer(ResidualBlock, 64,  2, stride=1)\n",
    "        self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=2)\n",
    "        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)   #strides=[1,1]\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inchannel, channels, stride))\n",
    "            self.inchannel = channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "\n",
    "    return ResNet(ResidualBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 10  #遍历数据集次数\n",
    "pre_epoch = 0  # 定义已经遍历数据集的次数\n",
    "BATCH_SIZE = 64      #批处理尺寸(batch_size)\n",
    "LR = 0.01        #学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet18().to(\"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()  #损失函数为交叉熵，多用于多分类问题nn.BCELoss()#\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4) \n",
    "#优化方式为mini-batch momentum-SGD，并采用L2正则化（权重衰减）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "\n",
    "best_acc = 85  #2 初始化best test accuracy\n",
    "print(\"Start Training, Resnet-18!\")  # 定义遍历数据集的次数\n",
    "\n",
    "for epoch in range(pre_epoch, EPOCH):\n",
    "    print('\\nEpoch: %d' % (epoch + 1))\n",
    "    net.train()\n",
    "    sum_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # 准备数据\n",
    "        length = len(trainloader)\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward\n",
    "        inputs = inputs.float()\n",
    "        outputs = net(inputs)\n",
    "        labels = labels.float()\n",
    "        labels = torch.topk(labels, 1)[1].squeeze(1)\n",
    "#         print(outputs[0])\n",
    "#         print(labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 每训练1个batch打印一次loss和准确率\n",
    "        sum_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "        print('[epoch:%d, iter:%d] Loss: %.03f | Acc: %.3f%% '\n",
    "              % (epoch + 1, (i + 1 + epoch * length), sum_loss / (i + 1), 100. * correct / total))\n",
    "\n",
    "# 每训练完一个epoch测试一下准确率\n",
    "        if i%50 ==0:\n",
    "            print(\"Waiting Test!\")\n",
    "            with torch.no_grad():\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                for data in testloader:\n",
    "                    net.eval()\n",
    "                    images, labels = data\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    labels = labels.float()\n",
    "                    labels = torch.topk(labels, 1)[1].squeeze(1)\n",
    "                    images = images.float()\n",
    "                    outputs = net(images)\n",
    "                    # 取得分最高的那个类 (outputs.data的索引号)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum()\n",
    "                    net.train()\n",
    "                print('测试分类准确率为：%.3f%%' % (100 * correct / total))\n",
    "        \n",
    "print(\"Training Finished, TotalEPOCH=%d\" % EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in testloader:\n",
    "        net.eval()\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.float()\n",
    "        labels = torch.topk(labels, 1)[1].squeeze(1)\n",
    "        images = images.float()\n",
    "        outputs = net(images)\n",
    "        # 取得分最高的那个类 (outputs.data的索引号)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    print('测试分类准确率为：%.3f%%' % (100 * correct / total))\n",
    "    acc = 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for data in testloader:\n",
    "    net.eval()\n",
    "    images, labels = data\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    labels = labels.float()\n",
    "    labels = torch.topk(labels, 1)[1].squeeze(1)\n",
    "    images = images.float()\n",
    "    outputs = net(images)\n",
    "    # 取得分最高的那个类 (outputs.data的索引号)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "print('测试分类准确率为：%.3f%%' % (100 * correct / total))\n",
    "acc = 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in testloader:\n",
    "    net.eval()\n",
    "    images, labels = data\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    labels = labels.float()\n",
    "    labels = torch.topk(labels, 1)[1].squeeze(1)\n",
    "    images = images.float()\n",
    "    outputs = net(images)\n",
    "    # 取得分最高的那个类 (outputs.data的索引号)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "print('测试分类准确率为：%.3f%%' % (100 * correct / total))\n",
    "acc = 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
